# SLM-Builder

**Build Small/Specialized Language Models from any dataset, source, or topic.**

SLM-Builder is an end-to-end Python toolkit for creating, training, and deploying specialized language models optimized for specific domains. Whether you have FAQ data, internal documentation, or customer support logs, SLM-Builder helps you build production-ready models with minimal ML expertise.

## âœ¨ Features

### Core Features
- ğŸ“¥ **Multiple Data Sources**: Load from CSV, JSONL, text files, URLs, databases (SQL, MongoDB), or REST APIs
- ğŸ¯ **Task-Specific**: Support for QA, classification, generation, and instruction-tuning
- ğŸš€ **Easy Training**: Pre-configured recipes (LoRA, full fine-tuning, instruction-tuning)
- ğŸ’» **CPU & GPU Support**: Optimized for both environments with hardware auto-detection
- ğŸ·ï¸ **Built-in Annotation**: Streamlit-based UI for data labeling
- ğŸ“¦ **Export Options**: ONNX, TorchScript, or HuggingFace format
- ğŸŒ **Production Ready**: FastAPI server template included
- ğŸ”’ **Security First**: PII detection and license checking

### Advanced Features
- ğŸ”€ **Dynamic Model Loading**: Load from HuggingFace Hub, Local paths, Ollama, GGUF files, HTTP/S3 URLs
- âš–ï¸ **Smart Dataset Splitting**: Train/val/test splits with stratification and K-fold cross-validation
- ğŸ—„ï¸ **Database Integration**: Direct loading from PostgreSQL, MySQL, SQLite, MongoDB
- ğŸŒ **API Data Loading**: REST API support with authentication and pagination
- ğŸ“Š **Model Comparison**: Benchmark multiple models with comprehensive metrics
- ğŸ“ˆ **Experiment Tracking**: Track hyperparameters, metrics, and model versions
- ğŸ”¬ **Advanced Evaluation**: Perplexity, BLEU, ROUGE, Accuracy, F1 scores
- âš¡ **Quantization**: 4-bit and 8-bit model quantization for efficiency
- ğŸ” **Dataset Validation**: Automatic quality checking and class balance analysis

ğŸ“– **[View Advanced Features Documentation â†’](FEATURES.md)**  
ğŸ“– **[View Additional Features Documentation â†’](ADDITIONAL_FEATURES.md)**

## ğŸš€ Quick Start

### Installation

```bash
# Basic installation (CPU-only)
pip install slm-builder

# Full installation (with GPU support)
pip install slm-builder[full]

# Development installation
pip install slm-builder[dev]
```

### Build Your First SLM

```python
from slm_builder import SLMBuilder

# Initialize builder
builder = SLMBuilder(project_name="faq-bot")

# Build from CSV in one line
result = builder.build_from_csv(
    path="data/faqs.csv",
    task="qa",
    recipe="lora"
)

print(f"Model saved to: {result['model_dir']}")
```

### CLI Usage

```bash
# Build from CSV
slm build --source data/faqs.csv --task qa --recipe lora --base-model gpt2

# Launch annotation UI
slm annotate --source data/raw.csv --task qa --out annotated.jsonl

# Export to ONNX
slm export --model output/best --format onnx --optimize cpu --quantize

# Serve the model
slm serve --model output/best --port 8080
```

## ğŸ“š Examples

### QA System from CSV

```python
from slm_builder import SLMBuilder

builder = SLMBuilder(
    project_name="customer-support",
    base_model="gpt2",
)

# CSV should have 'question' and 'answer' columns
result = builder.build_from_csv(
    path="support_qa.csv",
    task="qa",
    recipe="lora",
)
```

### Custom Preprocessing

```python
from slm_builder import SLMBuilder

def custom_filter(records):
    # Filter out short questions
    return [r for r in records if len(r.get("text", "")) > 20]

builder = SLMBuilder(project_name="my-slm")
builder.register_preprocessor(custom_filter)

result = builder.build_from_csv("data.csv", task="qa")
```

## ğŸ”§ Training Recipes

- **LoRA**: Efficient fine-tuning using Low-Rank Adaptation (recommended for CPU/limited resources)
- **Full Fine-tuning**: Traditional fine-tuning of all parameters (requires more resources)
- **Instruction-Tuning**: Specialized for instruction-following models

## ğŸ“ Configuration

Create a `config.yml`:

```yaml
project_name: my-slm
base_model: gpt2
task: qa
recipe: lora

preprocess:
  max_tokens_per_chunk: 512
  chunk_overlap: 64

training:
  batch_size: 8
  learning_rate: 5e-5
  epochs: 3

lora:
  r: 8
  lora_alpha: 32
  target_modules: [q_proj, v_proj]
```

## ğŸ—ï¸ Package Structure

```
slm_builder/
â”œâ”€â”€ api.py              # Main SLMBuilder class
â”œâ”€â”€ cli.py              # CLI commands
â”œâ”€â”€ data/               # Data loading and preprocessing
â”œâ”€â”€ models/             # Model training and export
â”œâ”€â”€ serve/              # FastAPI serving
â””â”€â”€ utils/              # Utilities
```

## ğŸ§ª Testing

```bash
pytest tests/
```

## ğŸ“„ License

MIT License - see LICENSE file

---

**Made with â¤ï¸ for building specialized AI models**